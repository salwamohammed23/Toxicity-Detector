import streamlit as st
import torch
import os
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel, PeftConfig

# 1. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.set_page_config(
    page_title="Ù†Ù…ÙˆØ°Ø¬ ÙƒØ´Ù Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø±",
    page_icon="âš ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)
st.title("âš ï¸ ÙƒØ§Ø´Ù Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© LoRA")

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
@st.cache_resource(show_spinner="Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
def load_model():
    try:
        model_path = "Model/lora_distilbert_toxic_final"
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ø¹ Ø±Ø³Ø§Ø¦Ù„ Ø®Ø·Ø£ Ù…Ø®ØµØµØ©
        required_files = {
            'adapter_config.json': "Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† LoRA Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ",
            'adapter_model.safetensors': "Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©",
            'tokenizer_config.json': "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Tokenizer",
            'vocab.txt': "Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"
        }
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        missing_files = []
        for file, desc in required_files.items():
            if not os.path.exists(os.path.join(model_path, file)):
                missing_files.append(f"- {desc} ({file})")
        
        if missing_files:
            raise FileNotFoundError(
                "Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø©:\n" + "\n".join(missing_files)
            )
        # ØªØ­Ù…ÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù…
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬..."):
            config = PeftConfig.from_pretrained(model_path)
        
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ..."):
            base_model = AutoModelForSequenceClassification.from_pretrained(
                config.base_model_name_or_path,
                num_labels=8,
                return_dict=True,
                ignore_mismatched_sizes=True,
                device_map="auto"
            )
        
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø¶Ø¨Ø· LoRA..."):
            model = PeftModel.from_pretrained(base_model, model_path)
        
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Tokenizer..."):
            tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        return model, tokenizer
        
    except Exception as e:
        st.error("## Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
        st.error(str(e))
        st.error("""
**Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø©:**
1. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­
2. ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
3. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„Ù…ÙƒØªØ¨Ø§Øª
4. Ø±Ø§Ø¬Ø¹ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„""")
        return None, None

model, tokenizer = load_model()

# 3. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù†Ø©
if model and tokenizer:
    st.sidebar.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
    
    # ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙØ¦Ø§Øª Ù…Ø¹ Ø£Ù„ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠØ©
    LABELS = {
        "ØºÙŠØ± Ø³Ø§Ù…": {"emoji": "âœ…", "color": "green"},
        "ÙƒØ±Ø§Ù‡ÙŠØ©": {"emoji": "ğŸ’¢", "color": "red"},
        "Ø¥Ù‡Ø§Ù†Ø©": {"emoji": "ğŸ—¯ï¸", "color": "orange"},
        "ØªÙ‡Ø¯ÙŠØ¯": {"emoji": "âš ï¸", "color": "red"},
        "Ø¹Ù†ØµØ±ÙŠ": {"emoji": "ğŸš«", "color": "red"},
        "Ø¬Ù†Ø³ÙŠ": {"emoji": "ğŸ”", "color": "red"},
        "ØªØ­Ø±ÙŠØ¶": {"emoji": "ğŸ”¥", "color": "orange"},
        "Ø£Ø®Ø±Ù‰": {"emoji": "â“", "color": "gray"}
    }
    
    with st.form("classification_form"):
        col1, col2 = st.columns([3, 1])
        with col1:
            text = st.text_area(
                "**Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡:**",
                height=200,
                placeholder="Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§...",
                help="ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø£ÙŠ Ù†Øµ Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆØ§Ù‡"
            )
        with col2:
            st.markdown("### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
            max_length = st.slider("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„", 128, 512, 256)
            threshold = st.slider("Ø­Ø¯ Ø§Ù„Ø«Ù‚Ø©", 0.0, 1.0, 0.7, 0.05)
        
        submitted = st.form_submit_button("**Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„**", use_container_width=True)
        
        if submitted and text:
            with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ..."):
                try:
                    # Tokenization Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
                    inputs = tokenizer(
                        text,
                        return_tensors="pt",
                        truncation=True,
                        padding=True,
                        max_length=max_length
                    ).to(model.device)
                    
                    # Ø§Ù„ØªÙ†Ø¨Ø¤
                    with torch.no_grad():
                        outputs = model(**inputs)
                        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                        pred_idx = torch.argmax(probs).item()
                        confidence = probs[0][pred_idx].item()
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    st.subheader("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
                    
                    pred_label = list(LABELS.keys())[pred_idx]
                    label_info = LABELS[pred_label]
                    
                    # Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                    if confidence > threshold:
                        st.success(f"""
                        ## {label_info['emoji']} Ø§Ù„ØªØµÙ†ÙŠÙ: **{pred_label}**  
                        **Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©**: {confidence:.2%}  
                        **Ø§Ù„ØªÙ‚ÙŠÙŠÙ…**: { "Ø®Ø·ÙŠØ±" if pred_idx > 0 else "Ø¢Ù…Ù†"}
                        """)
                    else:
                        st.warning(f"""
                        ## âš ï¸ ØªØµÙ†ÙŠÙ ØºÙŠØ± Ø­Ø§Ø³Ù…  
                        **Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹**: {pred_label}  
                        **Ø§Ù„Ø«Ù‚Ø©**: {confidence:.2%} (Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ {threshold:.0%})
                        """)
                    
                    # Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
                    st.markdown("### ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø§Øª")
                    for i, (label, prob) in enumerate(zip(LABELS.keys(), probs[0])):
                        prob_value = prob.item()
                        label_info = LABELS[label]
                        
                        cols = st.columns([1, 3, 1])
                        cols[0].markdown(f"**{label_info['emoji']} {label}**")
                        cols[1].progress(
                            prob_value,
                            text=f"{prob_value:.2%}"
                        )
                        cols[2].markdown(f"`{prob_value:.2%}`")
                    
                    # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø£Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø¹ØªØ¨Ø©
                    if confidence < threshold:
                        st.warning("""
                        **Ù…Ù„Ø§Ø­Ø¸Ø©**: Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰:
                        - Ù†Øµ ØºØ§Ù…Ø¶
                        - Ù„ØºØ© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©
                        - Ø³ÙŠØ§Ù‚ ØºÙŠØ± Ù…Ø­Ø¯Ø¯
                        """)
                        
                except Exception as e:
                    st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")
                    st.error("Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ ØºÙŠØ± ØµØ§Ù„Ø­")

# 4. Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
st.sidebar.markdown("## ğŸ› ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©")
st.sidebar.info("""
**ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:**
- **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**: DistilBERT-base-uncased
- **Ø§Ù„ØªÙ‚Ù†ÙŠØ©**: LoRA (Low-Rank Adaptation)
- **Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª**: 8
- **Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬**: ~70MB (Ù…Ø¹ LoRA)

**Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:**
- ÙƒØ´Ù Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø±
- ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³Ù…ÙŠØ©
- ØªØ­Ù„ÙŠÙ„ Ù„ØºØ© Ø§Ù„ÙƒØ±Ø§Ù‡ÙŠØ©
""")

st.sidebar.markdown("## ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
if model:
    st.sidebar.metric("Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª", len(LABELS))
    st.sidebar.metric("Ø­Ø¬Ù… Tokenizer", f"{len(tokenizer):,} Ù…ÙØ±Ø¯Ø©")

# 5. ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
footer = """
<style>
.footer {
    position: fixed;
    left: 0;
    bottom: 0;
    width: 100%;
    background-color: #f1f1f1;
    color: #555;
    text-align: center;
    padding: 10px;
    font-size: 12px;
}
</style>
<div class="footer">
    <p>ØªÙ… ØªØ·ÙˆÙŠØ±Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Transformers Ùˆ PEFT | Ø¥ØµØ¯Ø§Ø± v1.1.0</p>
</div>
"""
st.markdown(footer, unsafe_allow_html=True)
