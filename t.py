import streamlit as st
import torch
import numpy as np
import os
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer
)
from peft import PeftModel, PeftConfig

# 1. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.set_page_config(page_title="Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙŠ", layout="wide")
st.title("ğŸ¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LoRA")

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
@st.cache_resource
def load_model():
    try:
        model_path = "Model/lora_distilbert_toxic_final"
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        required_files = ['adapter_config.json', 'adapter_model.safetensors']
        for file in required_files:
            if not os.path.exists(os.path.join(model_path, file)):
                raise FileNotFoundError(f"Ù…Ù„Ù {file} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯")
        
        # ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª LoRA
        config = PeftConfig.from_pretrained(model_path)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        base_model = AutoModelForSequenceClassification.from_pretrained(
            config.base_model_name_or_path,
            num_labels=8,
            return_dict=True,
            ignore_mismatched_sizes=True
        )
        
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ LoRA
        model = PeftModel.from_pretrained(base_model, model_path)
        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        st.error("ØªØ£ÙƒØ¯ Ù…Ù†:")
        st.error("1. ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ 'Model/lora_distilbert_toxic_final'")
        st.error("2. Ø§Ø­ØªÙˆØ§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª adapter_config.json Ùˆ adapter_model.bin")
        st.error("3. ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
        return None, None

model, tokenizer = load_model()

# 3. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
if model and tokenizer:
    with st.form("text_classification"):
        text = st.text_area("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ù„ØªØµÙ†ÙŠÙ:", height=150, placeholder="Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§...")
        submitted = st.form_submit_button("ØµÙ†Ù‘Ù")
        
        if submitted and text:
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    truncation=True, 
                    padding=True,
                    max_length=512
                )
                
                # Ø§Ù„ØªÙ†Ø¨Ø¤
                with torch.no_grad():
                    outputs = model(**inputs)
                    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                    preds = torch.argmax(probs, dim=1).item()
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                st.subheader("ğŸ¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ")
                
                # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª (ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ø­Ø³Ø¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ)
                labels = [
                    "ØºÙŠØ± Ø³Ø§Ù…", "ÙƒØ±Ø§Ù‡ÙŠØ©", "Ø¥Ù‡Ø§Ù†Ø©",
                    "ØªÙ‡Ø¯ÙŠØ¯", "Ø¹Ù†ØµØ±ÙŠ", "Ø¬Ù†Ø³ÙŠ",
                    "ØªØ­Ø±ÙŠØ¶", "Ø£Ø®Ø±Ù‰"
                ]
                
                # Ø¹Ø±Ø¶ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                st.metric("Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ", labels[preds])
                
                # Ø¹Ø±Ø¶ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
                st.write("**ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:**")
                for i, (label, prob) in enumerate(zip(labels, probs[0])):
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        st.write(f"{label}:")
                    with col2:
                        st.progress(float(prob), text=f"{prob:.2%}")
                        
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØµÙ†ÙŠÙ: {str(e)}")
else:
    st.warning("ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø±Ø§Ø¬Ø¹ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø®Ø·Ø£ Ø£Ø¹Ù„Ø§Ù‡.")

# 4. Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø§Ù†Ø¨ÙŠØ©
st.sidebar.markdown("## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
st.sidebar.info("""
- **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:** DistilBERT
- **ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¶Ø¨Ø·:** LoRA (PEFT)
- **Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª:** 8
- **Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:**
  - Transformers: 4.33+
  - PEFT: 0.5+
  - PyTorch: 2.0+
""")

# 5. ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
st.markdown("""
<style>
.footer {
    font-size: 12px;
    text-align: center;
}
</style>
<div class="footer">
    ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit Ùˆ Hugging Face Transformers
</div>
""", unsafe_allow_html=True)
